# VRAM Requirements & Hardware Guide

## Quick Answer

**Can AuditWatch run on 4GB VRAM?**
- ‚ùå **NO** with default Llama 3.1 8B (needs 6GB minimum)
- ‚úÖ **YES** with smaller models (Llama 3.2 3B or Phi-3 Mini)

---

## Recommended Hardware

### Minimum (6GB VRAM) ‚≠ê RECOMMENDED
- **GPU:** NVIDIA RTX 3060 (6GB), RTX 2060 (6GB), or equivalent
- **Model:** Llama 3.1 8B (4-bit quantization)
- **Performance:** 2-3 minutes per narrative
- **Quality:** Production-ready

### Budget (4GB VRAM) ‚ö†Ô∏è ALTERNATIVE
- **GPU:** NVIDIA GTX 1650 (4GB), RTX 3050 (4GB), or equivalent
- **Model:** Llama 3.2 3B or Phi-3 Mini (4-bit quantization)
- **Performance:** 1-2 minutes per narrative
- **Quality:** Good for most cases

### Optimal (8GB+ VRAM) üöÄ BEST
- **GPU:** NVIDIA RTX 3070 (8GB), RTX 4060 Ti (8GB), or better
- **Model:** Llama 3.1 8B (8-bit quantization for better quality)
- **Performance:** 1-2 minutes per narrative
- **Quality:** Highest quality

---

## VRAM Requirements by Model

### Llama 3.1 8B (Default)

| Quantization | VRAM | Quality | Speed | Use Case |
|--------------|------|---------|-------|----------|
| **FP16** | 16GB | Excellent | Slow | Research only |
| **Q8** | 8-10GB | Very Good | Medium | Production (high quality) |
| **Q4_K_M** ‚≠ê | **6GB** | Good | Fast | **Production (recommended)** |
| **Q4_0** | 5-6GB | Acceptable | Fast | Budget production |

### Alternative Models for 4GB VRAM

| Model | Parameters | VRAM (Q4) | Quality | Recommended? |
|-------|-----------|-----------|---------|--------------|
| **Llama 3.2 3B** | 3B | 3-4GB | Good | ‚úÖ YES |
| **Phi-3 Mini** | 3.8B | 3-4GB | Excellent | ‚úÖ YES |
| **Gemma 2B** | 2B | 2-3GB | Basic | ‚ö†Ô∏è MAYBE |
| **TinyLlama** | 1.1B | 1-2GB | Poor | ‚ùå NO |

---

## How to Check Your VRAM

### Windows
```cmd
nvidia-smi
```
Look for "Memory-Usage" line

### Linux/Mac
```bash
nvidia-smi --query-gpu=memory.total --format=csv
```

### Alternative (if nvidia-smi not found)
- Open Task Manager ‚Üí Performance ‚Üí GPU ‚Üí Dedicated GPU Memory

---

## How to Switch Models

### For 4GB VRAM Users

**Step 1: Pull a smaller model**
```bash
# Option 1: Llama 3.2 3B (recommended)
ollama pull llama3.2:3b

# Option 2: Phi-3 Mini (Microsoft, excellent quality)
ollama pull phi3:mini

# Option 3: Gemma 2B (Google, basic)
ollama pull gemma:2b
```

**Step 2: Update config.yaml**
```yaml
llm:
  model: "llama3.2:3b"  # Change from "llama3.1:8b"
  temperature: 0.3
  max_tokens: 2000
  base_url: "http://localhost:11434"
```

**Step 3: Restart the app**
```bash
streamlit run src/ui/app.py
```

---

## Performance Comparison

### Narrative Generation Time

| Model | VRAM | Time per Case | Quality Score |
|-------|------|---------------|---------------|
| Llama 3.1 8B (Q8) | 8GB | 1-2 min | 95/100 |
| Llama 3.1 8B (Q4) | 6GB | 2-3 min | 90/100 ‚≠ê |
| Llama 3.2 3B (Q4) | 4GB | 1-2 min | 85/100 |
| Phi-3 Mini (Q4) | 4GB | 1-2 min | 88/100 |
| Gemma 2B (Q4) | 3GB | 1 min | 75/100 |

---

## CPU-Only Mode (No GPU)

**Can it run without a GPU?**
‚úÖ **YES** - but VERY slow (10-30 minutes per narrative)

**How to enable:**
```bash
# Ollama automatically falls back to CPU if no GPU detected
ollama run llama3.1:8b
```

**Recommended for:**
- Testing only
- Machines without GPU
- Not recommended for production

---

## Cloud/Server Options

### If you don't have enough VRAM:

**Option 1: Cloud GPU (Recommended)**
- **Google Colab** - Free T4 GPU (16GB VRAM)
- **Paperspace** - $0.51/hour for RTX 4000 (8GB)
- **RunPod** - $0.34/hour for RTX 3070 (8GB)

**Option 2: CPU-Only Server**
- AWS EC2 c6i.4xlarge (16 vCPU, 32GB RAM)
- Slow but works for low-volume processing

**Option 3: API-Based LLM**
- OpenAI GPT-4 (requires API key, data leaves system)
- Not recommended for banking compliance

---

## Troubleshooting

### Error: "Out of Memory"
**Solution:**
1. Check VRAM: `nvidia-smi`
2. Switch to smaller model: `ollama pull llama3.2:3b`
3. Update config.yaml with new model name
4. Restart app

### Error: "Model not found"
**Solution:**
```bash
# Pull the model first
ollama pull llama3.1:8b

# Verify it's installed
ollama list
```

### Slow Performance
**Possible causes:**
1. Running on CPU instead of GPU
2. Model too large for VRAM (swapping to RAM)
3. Other applications using GPU

**Solution:**
- Close other GPU applications (games, video editing)
- Use smaller model
- Check GPU usage: `nvidia-smi`

---

## Updated Pitch Deck Text

### Before (Incorrect):
> "Works on any machine with 8GB+ VRAM"

### After (Correct):

**Option 1: Minimum Requirement**
> "Works on any machine with **6GB+ VRAM** (NVIDIA RTX 3060 or equivalent)"

**Option 2: Flexible Requirement**
> "Flexible hardware requirements: 6GB VRAM for optimal quality (Llama 3.1 8B), or 4GB VRAM with smaller models (Llama 3.2 3B, Phi-3 Mini)"

**Option 3: Detailed**
> "Minimum: 6GB VRAM (RTX 3060) for production quality. Budget option: 4GB VRAM (GTX 1650) with smaller models. Optimal: 8GB+ VRAM (RTX 3070) for best performance."

---

## Recommended Pitch Deck Update

### Scalability Section:

**Current:**
> "MVP handles single-analyst workflows. Works on any machine with 8GB+ VRAM."

**Updated:**
> "MVP handles single-analyst workflows. **Minimum 6GB VRAM** (NVIDIA RTX 3060 or equivalent) for Llama 3.1 8B. **Alternative: 4GB VRAM** with smaller models (Llama 3.2 3B, Phi-3 Mini) for budget deployments. CPU-only mode available but not recommended for production."

---

## Summary

| VRAM | Model | Status | Use Case |
|------|-------|--------|----------|
| **8GB+** | Llama 3.1 8B (Q8) | ‚úÖ Optimal | Production (best quality) |
| **6GB** | Llama 3.1 8B (Q4) | ‚úÖ Recommended | Production (good quality) ‚≠ê |
| **4GB** | Llama 3.2 3B / Phi-3 Mini | ‚úÖ Alternative | Budget production |
| **<4GB** | CPU-only | ‚ö†Ô∏è Not Recommended | Testing only |

**Bottom Line:** 
- **6GB VRAM minimum** for production with default model
- **4GB VRAM works** with smaller models (slight quality trade-off)
- **8GB+ VRAM optimal** for best quality and speed

---

## References

1. [Ollama VRAM Requirements Guide](https://localllm.in/blog/ollama-vram-requirements-for-local-llms) - Entry-level (3-4GB VRAM) can run 3-4B models; mid-range (6-8GB) supports Llama 3.1 8B at Q4 quantization
2. [HuggingFace Discussion](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/discussions/77) - Llama 3.1 8B needs 6GB for 4-bit quantization inference
3. [LM Studio VRAM Guide](https://localllm.in/blog/lm-studio-vram-requirements-for-local-llms) - 4-6GB VRAM can run 3-4B models; 8-12GB supports 7-14B models

Content rephrased for compliance with licensing restrictions.
